# ğŸ¬ Korean Movie Review Sentiment Analyzer

A machine learning project that fine-tunes a pretrained Korean language model ([KoELECTRA](https://huggingface.co/monologg/koelectra-base-finetuned-sentiment)) on the **NSMC dataset** (200,000 Korean movie reviews) to classify reviews as **positive** or **negative**.

> Built as a portfolio project to demonstrate practical NLP and transfer learning skills.

---

## ğŸ“Œ Table of Contents

- [Overview](#overview)
- [How It Works](#how-it-works)
- [Dataset](#dataset)
- [Model Architecture](#model-architecture)
- [Results](#results)
- [Installation & Usage](#installation--usage)
- [Project Structure](#project-structure)
- [Key Concepts Explained](#key-concepts-explained)
- [What I Learned](#what-i-learned)

---

## Overview

**Goal:** Given a Korean movie review (plain text), predict whether it expresses a **positive** (ğŸ‘) or **negative** (ğŸ‘) sentiment.

**Approach:** Instead of training a model from scratch (which requires massive data and compute), I used **transfer learning** â€” taking a model that already understands Korean and teaching it to classify sentiment.

**Example:**
```
Input:  "ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì–´ìš”! ë°°ìš°ë“¤ ì—°ê¸°ë„ ìµœê³ !"
Output: Positive âœ… (confidence: 92.3%)

Input:  "ì™„ì „ ë³„ë¡œ... ì‹œê°„ ë‚­ë¹„í–ˆë‹¤."
Output: Negative âŒ (confidence: 88.7%)
```

---

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING PIPELINE                        â”‚
â”‚                                                             â”‚
â”‚  Korean Review Text                                         â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Converts text to numbers                   â”‚
â”‚  â”‚Tokenizer â”‚    "ì¢‹ì•„ìš”" â†’ [2, 1378, 8834, 3]              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                               â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Pretrained Korean language model        â”‚
â”‚  â”‚  KoELECTRA   â”‚    Already understands Korean grammar,    â”‚
â”‚  â”‚  (ELECTRA)   â”‚    vocabulary, and context                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Added on top of KoELECTRA              â”‚
â”‚  â”‚ Classifier   â”‚    Learns to map language understanding   â”‚
â”‚  â”‚   Head       â”‚    â†’ positive/negative                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚       â”‚                                                     â”‚
â”‚       â–¼                                                     â”‚
â”‚  Prediction: [0.15, 0.85] â†’ Positive (85% confidence)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Dataset

**NSMC (Naver Sentiment Movie Corpus)** â€” one of the most widely used Korean NLP benchmark datasets.

| Split    | Samples  | Description                          |
|----------|----------|--------------------------------------|
| Training | 150,000  | Reviews used to train the model     |
| Test     | 50,000   | Reviews used to evaluate the model  |

- **Source:** [github.com/e9t/nsmc](https://github.com/e9t/nsmc)
- **Labels:** Binary â€” `0` (negative) and `1` (positive)
- **Balance:** Nearly 50/50 split (well-balanced)

### Data Cleaning
Before training, the data is cleaned:
- âŒ **5 missing values** removed (empty review text)
- âŒ **3,817 duplicate reviews** removed from training set
- âŒ **Empty strings** (whitespace-only reviews) removed
- âœ… Final training set: **146,182 reviews**

---

## Model Architecture

### Why KoELECTRA?

| Model | Pros | Cons |
|-------|------|------|
| Train from scratch | Full control | Needs millions of samples, weeks of training |
| **KoELECTRA (chosen)** | **Already understands Korean, fast to fine-tune** | Requires GPU for full training |
| KR-FinBert-SC | Good at Korean | Designed for financial text, not general sentiment |

**KoELECTRA** is an [ELECTRA](https://arxiv.org/abs/2003.10555)-based model pretrained on a large Korean text corpus. ELECTRA models are trained using a "replaced token detection" approach, which is more sample-efficient than BERT's masked language modeling.

### Fine-tuning Process
```
Pretrained KoELECTRA (knows Korean)
        +
Classification Head (2 outputs: negative, positive)
        â†“
Train on NSMC for up to 10 epochs (early stopping on eval_loss)
        â†“
Fine-tuned Sentiment Classifier (90.2% accuracy)
```

### Training Hyperparameters
| Parameter | Value | Why |
|-----------|-------|-----|
| Learning rate | 2e-5 | Standard for fine-tuning transformers |
| Batch size | 32 | Balance between speed and memory |
| Max epochs | 10 (early stopping) | Stops automatically when eval_loss stops improving |
| Early stopping patience | 2 | Stop if no improvement for 2 evaluations in a row |
| Max token length | 128 | Most Korean reviews are shorter than this |
| Warmup steps | 100 | Prevents unstable early training |
| Weight decay | 0.01 | Regularization to prevent overfitting |

---

## Results

*Trained on the full dataset: **146,182 Korean movie reviews** (after cleaning).*

| Metric    | Score  |
|-----------|--------|
| Accuracy  | **90.2%** |
| Precision | **90.2%** |
| Recall    | **90.3%** |
| F1 Score  | **90.3%** |

This result is competitive with published results on the NSMC benchmark (typical range: 88â€“92%).

### Sample Predictions
| Review (Korean) | Translation | Predicted | Confidence |
|-----------------|-------------|-----------|------------|
| ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì–´ìš”! ë°°ìš°ë“¤ ì—°ê¸°ë„ ìµœê³ ! | This movie is really fun! The acting is the best! | âœ… Positive | 99.4% |
| ì™„ì „ ë³„ë¡œ... ì‹œê°„ ë‚­ë¹„í–ˆë‹¤. | Totally bad... waste of time. | âŒ Negative | 99.5% |
| ì—­ëŒ€ ìµœê³ ì˜ í•œêµ­ ì˜í™”! ê¼­ ë³´ì„¸ìš”! | Best Korean movie ever! Must watch! | âœ… Positive | 99.3% |
| ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ì§€ë£¨í•˜ê³  ì—°ê¸°ê°€ ì–´ìƒ‰í•´ìš”. | The story is boring and the acting is awkward. | âŒ Negative | 99.5% |

---

## Installation & Usage

### Prerequisites
- Python 3.9+
- ~2GB disk space (for model weights and data)

### Setup
```bash
# Clone the repository
git clone https://github.com/cringepnh/korean-sentiment-analyzer.git
cd korean-sentiment-analyzer

# Create virtual environment (recommended)
python -m venv .venv
source .venv/bin/activate    # Linux/Mac
.venv\Scripts\activate       # Windows

# Install dependencies
pip install -r requirements.txt
```

### Run the Full Pipeline
```bash
python main.py
```

This will:
1. Load the NSMC dataset (200k Korean movie reviews)
2. Clean and preprocess the data
3. Tokenize reviews using KoELECTRA's tokenizer
4. Fine-tune the model on the full 146k training set
5. Evaluate and print metrics (accuracy, F1, confusion matrix)
6. Test on sample Korean reviews
7. Save the trained model to `models/`

> ğŸ’¡ To do a quick test run first, set `FULL_TRAINING = False` in `main.py` â€” trains on 5,000 samples in ~5 minutes.

> ğŸ’¡ Training supports **checkpoint resume** â€” if you stop and restart, it continues from the last saved checkpoint automatically.

### Use as Standalone Predictor
After training, you can use the model directly in Python:
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from main import predict_sentiment

# Load the saved model
model = AutoModelForSequenceClassification.from_pretrained("models/sentiment-model")
tokenizer = AutoTokenizer.from_pretrained("models/sentiment-model")

# Predict sentiment for any Korean text
result = predict_sentiment("ì´ ì˜í™” ì •ë§ ì¢‹ì•„ìš”!", model, tokenizer)
print(result)
# {'text': 'ì´ ì˜í™” ì •ë§ ì¢‹ì•„ìš”!', 'sentiment': 'Positive âœ…', 'confidence': 0.94, 'label': 1}
```

---

## Project Structure

```
korean-sentiment-analyzer/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ratings_train.txt       # 150k training reviews (TSV)
â”‚   â””â”€â”€ ratings_test.txt        # 50k test reviews (TSV)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ sentiment-model/        # Saved trained model (generated after training)
â”œâ”€â”€ notebooks/                  # Jupyter notebooks (for experimentation)
â”œâ”€â”€ main.py                     # Complete ML pipeline (all 8 steps)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

### Key Files

| File | Description |
|------|-------------|
| `main.py` | The complete pipeline â€” from data loading to model saving |
| `requirements.txt` | All required Python packages |
| `data/ratings_train.txt` | 150,000 labeled Korean movie reviews for training |
| `data/ratings_test.txt` | 50,000 labeled Korean movie reviews for evaluation |

---

## Key Concepts Explained

### What is Transfer Learning?
Instead of training a model from scratch on our small dataset, we take a model that was already trained on millions of Korean sentences (KoELECTRA) and **fine-tune** it for our specific task. This is like hiring a Korean language expert and teaching them to do movie review analysis â€” much faster than teaching someone Korean from scratch.

### What is Tokenization?
Neural networks work with numbers, not text. A **tokenizer** converts Korean text into sequences of numbers using a learned vocabulary:
```
"ì´ ì˜í™” ì¢‹ì•„ìš”" â†’ [2, 1378, 2495, 8834, 3]
```
Each number corresponds to a piece of a word (called a "subword token") in the model's vocabulary.

### What is Fine-tuning?
Fine-tuning adjusts the pretrained model's weights slightly so it becomes good at our specific task (sentiment classification). We use a small learning rate (2e-5) to make tiny adjustments without "forgetting" the Korean language knowledge.

### Evaluation Metrics
- **Accuracy**: Percentage of correct predictions overall
- **Precision**: Of all reviews predicted positive, how many actually are?
- **Recall**: Of all actually positive reviews, how many did the model find?
- **F1 Score**: Harmonic mean of precision and recall â€” a balanced single metric
- **Confusion Matrix**: A 2Ã—2 grid showing exact counts of correct/incorrect predictions

---

## What I Learned

Building this project taught me:

1. **NLP Pipeline Design** â€” How to structure an end-to-end machine learning project: data loading â†’ cleaning â†’ preprocessing â†’ training â†’ evaluation â†’ deployment.

2. **Transfer Learning** â€” Why fine-tuning pretrained models (like KoELECTRA) is more practical than training from scratch, especially with limited compute resources.

3. **Data Cleaning** â€” Real-world data is messy. Handling missing values, duplicates, and edge cases is a critical (and often underestimated) step.

4. **Tokenization** â€” How transformer models convert text into numerical representations, and why subword tokenization works well for Korean.

5. **HuggingFace Ecosystem** â€” Practical experience with the `transformers` and `datasets` libraries, which are industry-standard tools for NLP.

6. **Model Evaluation** â€” Understanding that accuracy alone isn't enough â€” precision, recall, and F1 give a fuller picture of model performance.

7. **Training Best Practices** â€” Learning rate warmup, weight decay, checkpoint saving, and how to resume interrupted training.

---

## Technologies Used

- **Python 3** â€” Primary programming language
- **PyTorch** â€” Deep learning framework
- **HuggingFace Transformers** â€” Pre-trained model library
- **KoELECTRA** â€” Korean ELECTRA language model
- **Pandas & NumPy** â€” Data manipulation
- **scikit-learn** â€” Evaluation metrics
- **NSMC Dataset** â€” Korean sentiment benchmark

---

## License

This project is open source and available under the [MIT License](LICENSE).

---

*Built with â¤ï¸ as a machine learning portfolio project*
